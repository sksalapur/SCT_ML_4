# ğŸ¯ Hand Gesture Recognition

A real-time AI-powered hand gesture recognition system built with TensorFlow and Streamlit.

## ğŸš€ [Live Demo](https://your-app-name.streamlit.app) 

[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://your-app-name.streamlit.app)

## âœ¨ Features

- **Real-time Recognition**: Live webcam gesture detection
- **8 Gesture Classes**: Supports fist, palm, index, ok, thumb, c_shape, down, l_shape
- **97.9% Accuracy**: High-performance deep learning model
- **Web Interface**: Easy-to-use Streamlit web app
- **Image Upload**: Test with your own gesture images

## ğŸ¯ Supported Gestures

| Gesture | Description | Use Case |
|---------|-------------|----------|
| ğŸ‘Š Fist | Closed fist | Stop/Select |
| ğŸ‘‹ Palm | Open palm | Hello/Stop |
| ğŸ‘† Index | Pointing finger | Direction/Select |
| ğŸ‘Œ OK | OK sign | Confirm/Good |
| ğŸ‘ Thumb | Thumbs up | Like/Approve |
| ğŸ¤ C-Shape | C-shaped hand | Grab/Pinch |
| ğŸ‘‡ Down | Pointing down | Navigate down |
| ğŸ¤Ÿ L-Shape | L-shaped hand | Custom action |

## ğŸ”§ Technology Stack

- **Deep Learning**: TensorFlow 2.x + Keras
- **Model**: MobileNetV2 (Transfer Learning)
- **Computer Vision**: OpenCV
- **Web Framework**: Streamlit
- **Deployment**: Streamlit Cloud

## ğŸ“Š Model Performance

- **Training Accuracy**: 97.2%
- **Validation Accuracy**: 97.9%
- **Model Size**: 2.4M parameters
- **Inference Speed**: ~50ms per prediction
- **Dataset**: 5,120 training images (balanced)

## ğŸš€ Quick Start

### Option 1: Use the Web App (Recommended)
Visit the [live demo](https://your-app-name.streamlit.app) and start using immediately!

### Option 2: Run Locally

1. **Clone the repository**
```bash
git clone https://github.com/YOUR_USERNAME/gesture-recognition.git
cd gesture-recognition
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Download the model** (auto-downloaded on first run)

4. **Run the app**
```bash
streamlit run streamlit_app.py
```

## ğŸ“ Project Structure

```
gesture-recognition/
â”œâ”€â”€ streamlit_app.py          # Main Streamlit application
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                # Project documentation
â”œâ”€â”€ .gitignore               # Git ignore file
â”œâ”€â”€ setup.py                 # Package setup
â””â”€â”€ models/
    â””â”€â”€ download_model.py    # Model download script
```

## ğŸ® Usage

### Web Interface
1. **Image Upload Mode**: Upload a gesture image and get instant predictions
2. **Webcam Mode**: Enable live gesture recognition through your camera
3. **Confidence Scores**: View prediction confidence for all gesture classes

### API Usage (Advanced)
```python
from streamlit_app import load_gesture_model, predict_gesture
import cv2

# Load model
model, status = load_gesture_model()

# Make prediction
image = cv2.imread("gesture_image.jpg")
predicted_class, confidence, all_predictions = predict_gesture(model, image)

print(f"Predicted: {predicted_class} ({confidence:.2f})")
```

## ğŸ”¬ Model Training

The model was trained using:
- **Dataset**: LeapGestRecog dataset (balanced to 640 images per class)
- **Architecture**: MobileNetV2 with custom head
- **Training**: Two-phase training (frozen â†’ fine-tuning)
- **Augmentation**: Rotation, zoom, shift, brightness adjustment
- **Optimization**: Adam optimizer with learning rate scheduling

## ğŸ“ˆ Performance Metrics

| Gesture | Precision | Recall | F1-Score |
|---------|-----------|--------|----------|
| c_shape | 0.952 | 1.000 | 0.976 |
| down | 1.000 | 0.975 | 0.987 |
| fist | 0.981 | 0.975 | 0.978 |
| index | 1.000 | 1.000 | 1.000 |
| l_shape | 0.889 | 1.000 | 0.941 |
| ok | 1.000 | 0.912 | 0.954 |
| palm | 1.000 | 0.981 | 0.991 |
| thumb | 1.000 | 0.988 | 0.994 |

## ğŸ› ï¸ Development

### Setup Development Environment
```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/gesture-recognition.git
cd gesture-recognition

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run locally
streamlit run streamlit_app.py
```

### Testing
```bash
python -m pytest tests/
```

## ğŸš€ Deployment

### Streamlit Cloud (Recommended)
1. Fork this repository
2. Connect to [Streamlit Cloud](https://streamlit.io/cloud)
3. Deploy directly from GitHub
4. Your app will be live at `https://your-app-name.streamlit.app`

### Docker
```bash
docker build -t gesture-recognition .
docker run -p 8501:8501 gesture-recognition
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Dataset**: LeapGestRecog dataset from Kaggle
- **Inspiration**: Advances in computer vision and HCI

## ğŸ“ Contact

- **GitHub**: [@YOUR_USERNAME](https://github.com/YOUR_USERNAME)
- **Email**: your.email@example.com
- **Live Demo**: [gesture-recognition.streamlit.app](https://your-app-name.streamlit.app)

---

**â­ Star this repository if you found it helpful!**
